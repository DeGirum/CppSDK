<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.17"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>DeGirum AI Client for C++ Reference Guide: DeGirum AI Client for C++</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="degirum64.png"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">DeGirum AI Client for C++ Reference Guide
   &#160;<span id="projectnumber">v1.0.1</span>
   </div>
   <div id="projectbrief">DeGirum AI Client for C++ Reference Guide</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.17 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
</div><!-- top -->
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="PageDoc"><div class="header">
  <div class="headertitle">
<div class="title">DeGirum AI Client for C++ </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h1><a class="anchor" id="sec_intro"></a>
Introduction</h1>
<p>The DeGirum AI Client package referenced in this document is the part of DeGirum AI Software Suite.</p>
<p><em>Note: AI Client package consists of multiple programming language support packs. This document describes AI Client for C++. AI client packs for other languages are described in separate documents.</em></p>
<p>The DeGirum AI Software Suite provides means to perform fast and efficient AI model inferencing on locally installed AI hardware accelerator platforms.</p>
<p>The list of supported hardware accelerator platforms includes all DeGirum Orca AI hardware accelerators and Google Edge TPU accelerators. AI inferencing on local host CPU is also supported.</p>
<p>The DeGirum AI Software Suite consists of DeGirum AI Server package, AI Model Zoo, and DeGirum AI Client package.</p>
<h2><a class="anchor" id="ssec_server"></a>
AI Server Package</h2>
<p>The AI Server package is installed on a computer system where AI hardware accelerators are connected to. The purpose of the AI Server is the following:</p>
<ul>
<li>It controls AI hardware accelerators</li>
<li>It provides distributed and concurrent access of multiple AI clients to AI inferencing functionality of AI hardware accelerators over the TCP/IP network</li>
<li>It maintains local AI model zoo with AI models to be used for AI inferencing</li>
<li>It provides Web-based GUI to manage local model zoo</li>
</ul>
<p>The following computer platforms are currently supported for the server installation:</p>
<ul>
<li>Intel x64 with Linux 64-bit OS</li>
<li>ARM64 with Linux 64-bit OS</li>
</ul>
<p>The support for following computer platforms is currently under development:</p><ul>
<li>Intel x64 with Windows 10 OS</li>
<li>Intel x64 with MacOS</li>
<li>ARM64 with MacOS</li>
</ul>
<p>The AI Server for Linux is distributed as fully configured Docker container (refer to <a href="https://hub.docker.com/r/degirum/aiserver">https://hub.docker.com/r/degirum/aiserver</a>), which simplifies AI Server deployment to the variety of Linux platforms.</p>
<h2><a class="anchor" id="ssec_zoo"></a>
AI Model Zoo</h2>
<p>The AI Model Zoo is the place where AI models reside.</p>
<p>The local AI Model Zoo of DeGirum AI Server is implemented as the directory on the AI Server local file system. <br  />
 It contains files of all locally deployed AI models. Only models in the local Model Zoo can be used for AI inferencing on particular AI server installation.</p>
<p>Currently, AI Server supports the following AI model types:</p>
<ul>
<li>N2X models: models in proprietary DeGirum N2X format compiled for DeGirum Orca AI accelerator hardware and CPU</li>
<li>TFLite Edge TPU models: models in Google TFLite format compiled for Google Edge TPU AI accelerator hardware</li>
<li>TFLite CPU models: models in Google TFLite format compiled for execution on CPU</li>
<li>OpenVINO models</li>
</ul>
<p>The deployment of the local Model Zoo is performed using PySDK server module (please refer to PySDK documentation available on <a href="https://degirum.github.io">https://degirum.github.io</a>).</p>
<h2><a class="anchor" id="ssec_client"></a>
AI Client Package</h2>
<p>The AI Client package is installed on a computer system, where client-side ML software runs. It can be the same computer system, where the AI Server is deployed, or it can be any other computer system, which has TCP/IP network access to the AI Server computer system.</p>
<p>AI Client Package includes the following components:</p>
<ul>
<li>AI Client library</li>
<li>AI Client examples</li>
<li>Build scripts</li>
</ul>
<p>AI Client library provides APIs to control and use the AI Server. It includes the following functionality:</p>
<ul>
<li>List all AI models with their attributes available for AI inferencing in particular AI Server local Model Zoo</li>
<li>Find AI model in that local Model Zoo matching particular model attributes</li>
<li>Open and configure streaming connection to particular AI Server for inferencing using particular AI model from local Model Zoo</li>
<li>Stream data frames to the that streaming connection to perform AI inferencing on these data frames</li>
</ul>
<p>AI Client library provides integration into the C++ programming language. This document describes AI Client for C++. AI client packs for other languages are described in separate documents.</p>
<p>All AI Client Package software is distributed in the source code and requires compilation.</p>
<p>The following computer platforms are currently supported for the client installation:</p>
<ul>
<li>Intel x64 with Linux 64-bit OS</li>
<li>Intel x64 with Windows 10 OS</li>
<li>ARM64 with Linux 64-bit OS</li>
<li>Intel x64 with MacOS</li>
<li>ARM64 with MacOS</li>
</ul>
<h1><a class="anchor" id="sec_client_build"></a>
AI Client Package Installation and Build</h1>
<h2><a class="anchor" id="ssec_prerequisites"></a>
Installation Pre-Requisites</h2>
<p>For Linux OS the following software packages need to be installed in order to build AI Client package software:</p><ul>
<li>Git</li>
<li>CMake</li>
<li>GCC compiler</li>
<li>Thread building blocks library</li>
</ul>
<p>To install all these component use the following commands: </p><pre class="fragment">    sudo apt install git-all cmake g++ libtbb-dev
</pre><p>For Windows OS the following software packages need to be installed in order to build AI Client package software:</p><ul>
<li>Git</li>
<li>CMake</li>
<li>Visual Studio 2019 (Community Edition would suffice)</li>
</ul>
<h2><a class="anchor" id="ssec_install"></a>
AI Client Package Installation</h2>
<p>The AI Client library is distributed in source code and is available on GitHub at the following link: </p><pre class="fragment">    https://github.com/DeGirum/CppSDK
</pre><p>You clone the AI Client library repo by executing command: </p><pre class="fragment">    git clone https://github.com/DeGirum/CppSDK
</pre><p>The repo directory contains the AI Client library collaterals. The subdirectory structure is as follows:</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone">Directory </th><th class="markdownTableHeadNone">Contents  </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone"><code>/client</code> </td><td class="markdownTableBodyNone">AI Client library code  </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone"><code>/doc</code> </td><td class="markdownTableBodyNone">documentation  </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone"><code>/examples</code> </td><td class="markdownTableBodyNone">sample applications  </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone"><code>/extern</code> </td><td class="markdownTableBodyNone">third-party libraries  </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone"><code>/images</code> </td><td class="markdownTableBodyNone">sample images to be used for AI inference  </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone"><code>/inc</code> </td><td class="markdownTableBodyNone">AI Client library C++ include files  </td></tr>
</table>
<h2><a class="anchor" id="ssec_build"></a>
AI Client Package Build</h2>
<p>The CMakeLists.txt file is provided for the CMake build. It builds AI Client C++ library, and C++ examples.</p>
<p>To perform the build under Linux OS, execute the following commands: </p><pre class="fragment">    cd client
    mkdir build
    cd build
    cmake ..
    cmake --build .
</pre><p>For Windows OS and Visual Studio, CMake will create the Visual Studio project files and <code>AICLIENTLIB.sln</code> solution.</p>
<p>For Linux OS, CMake will create make files (unless otherwise specified with the -G flag).</p>
<p>After building, the library and example executables are copied into <code>/bin</code> subdirectory.</p>
<h1><a class="anchor" id="sec_api"></a>
AI Client C++ API Description</h1>
<p>DeGirum AI Client library provides the following functionality:</p>
<ul>
<li><a class="el" href="namespaceDG.html#ae8feea95766cbc4474d6d9c0601b2fd6">versionGet()</a> function to query the version of the AI Client library</li>
<li><a class="el" href="namespaceDG.html#a482a6ce0ec392df3b6f26db70b9b941e">modelzooListGet()</a> function to list all AI models with their attributes available for AI inferencing in particular AI Server local Model Zoo</li>
<li><a class="el" href="namespaceDG.html#ae467210f5bf315ec3c8e64ebf8f28dd4">systemInfo()</a> function to obtain AI server system information including information about connected AI devices</li>
<li><a class="el" href="namespaceDG.html#ae6fb2ad5460b7160220feb74635845a1">modelFind()</a> function to find AI model in that local Model Zoo matching particular model attributes</li>
<li><a class="el" href="namespaceDG.html#acac45c44fa4c75613f425b56561c2e84">labelDictionary()</a> function to query the model class label dictionary</li>
<li><a class="el" href="namespaceDG.html#a9929b406008257b1ff51b5470aa094af">errorCheck()</a> function to check given AI server JSON response for run-time errors</li>
<li><a class="el" href="classDG_1_1AIModel.html" title="AIModel is DeGirum AI client API class for simple non-pipelined sequential inference.">DG::AIModel</a> class to connect to AI server and to perform AI model inference in a simple non-pipelined sequential manner (see <a class="el" href="index.html#ssec_AIModel">AIModel: Synchronous Sequential AI Inferencing API</a>)</li>
<li><a class="el" href="classDG_1_1AIModelAsync.html" title="AIModelAsync is DeGirum AI client API class for efficient pipelined asynchronous inference.">DG::AIModelAsync</a> class to connect to AI server and to perform AI model inference in efficient pipelined asynchronous manner using mechanism of callbacks (see <a class="el" href="index.html#ssec_AIModelAsync">AIModelAsync: Asynchronous Pipelined AI Inferencing API</a>)</li>
<li>the family of ModelParams classes to handle model parameters (see <a class="el" href="index.html#ssec_params">Model Parameters Management</a>)</li>
<li>functions and methods for error handling (see <a class="el" href="index.html#ssec_errors">Error Handling</a>)</li>
</ul>
<h2><a class="anchor" id="ssec_server_addressing"></a>
AI Server Connection and Addressing</h2>
<p>The client software communicates with AI server through TCP/IP network using DeGirum proprietary communication protocol based on ASIO library. In order to establish a connection to AI server, the client side must know the host name or IP address of the AI server, and TCP port, which AI server listens.</p>
<p>The server address is passed to AI inference classes as a string in the following format:</p>
<p>If AI server domain name is known, the address string is specified in the form </p><pre class="fragment">    domain_name:port
</pre><p>If only IP address of AI server is known, the address string is specified in the form </p><pre class="fragment">    xxx.xxx.xxx.xxx:port
</pre><p>The port number together with ':' separator can be omitted, in this case default port 8778 is used. When AI server is started without specifying the port, the same default port 8778 is used for the server side. In this case it is safe to omit the port. Otherwise you should match the port on the client side to the port used on the server side.</p>
<h2><a class="anchor" id="ssec_zoo_search"></a>
Listing and Searching Models in Local Model Zoo</h2>
<p>The AI server can perform AI inferences only on models located either in the local model zoo of that AI server or in the cloud zoo you have access to. This section describes how to deal with local zoo models. The section <a class="el" href="index.html#ssec_cloud_zoo">Working with Cloud Zoo Models</a> describes how to deal with cloud zoo models.</p>
<p>To initiate an inference of a model form the local model zoo on AI server, the client must provide to the server the <em>model name</em>, which uniquely identifies the desired model in the local model zoo. There are two ways for the client code to obtain the model name.</p><ol type="1">
<li>By obtaining the full list of models available in the model zoo using <a class="el" href="namespaceDG.html#a482a6ce0ec392df3b6f26db70b9b941e">modelzooListGet()</a> function</li>
<li>By searching the model zoo for a model satisfying certain search attributes using <a class="el" href="namespaceDG.html#ae6fb2ad5460b7160220feb74635845a1">modelFind()</a> function.</li>
</ol>
<p>Either way you obtain the instance of <a class="el" href="structDG_1_1ModelInfo.html" title="ModelInfo is the model identification structure. It keeps AI model key attributes.">DG::ModelInfo</a> structure, which contains all necessary model identification information, including:</p><ul>
<li><a class="el" href="structDG_1_1ModelInfo.html#a26d97c4f78710a202b712af3e1b04332" title="model string name">DG::ModelInfo::name</a> - model string name</li>
<li><a class="el" href="structDG_1_1ModelInfo.html#a540d9fa4d6af0a6b9f7fb49c0e087e91" title="extended model parameters">DG::ModelInfo::extended_params</a> - extended model parameters (all parameter as listed in model JSON configuration file)</li>
</ul>
<p>Then you pass model name <a class="el" href="structDG_1_1ModelInfo.html#a26d97c4f78710a202b712af3e1b04332" title="model string name">DG::ModelInfo::name</a> to constructors of classes used for AI inference: <a class="el" href="classDG_1_1AIModel.html" title="AIModel is DeGirum AI client API class for simple non-pipelined sequential inference.">DG::AIModel</a> or <a class="el" href="classDG_1_1AIModelAsync.html" title="AIModelAsync is DeGirum AI client API class for efficient pipelined asynchronous inference.">DG::AIModelAsync</a>.</p>
<p>The <a class="el" href="namespaceDG.html#a482a6ce0ec392df3b6f26db70b9b941e">modelzooListGet()</a> function returns an array of ModelInfo structures listing all models found in the local model zoo. It is then your responsibility to iterate over all elements of this array to find the model you want to use.</p>
<p>The <a class="el" href="namespaceDG.html#ae6fb2ad5460b7160220feb74635845a1">modelFind()</a> function provides simpler means to find the desired model. It accepts the model query: the instance of <a class="el" href="structDG_1_1ModelQuery.html" title="Model query structure used to search models on AI server which match a set of provided model attribut...">DG::ModelQuery</a> structure, which contains some model attributes you want to look for. The <a class="el" href="namespaceDG.html#ae6fb2ad5460b7160220feb74635845a1">modelFind()</a> function first obtains the full list of models from the AI server, and then iterates over this list to find a model matching all attributes specified in the model query. It returns the first model matching all attributes.</p>
<p>You may specify the following attributes:</p><ul>
<li>Any part of the model name. Only models which have the specified substring in their model name would be considered.</li>
<li>Device type to use for inference. Only models which are designed for specified device type would be considered.</li>
<li>Runtime agent to use for inference. Only models which are designed for specified runtime agent would be considered.</li>
<li>Model should be quantized flag. Only quantized models would be considered.</li>
<li>Model should be pruned/sparse flag. Only sparse/pruned models would be considered.</li>
</ul>
<p>Only the model name substring attribute is mandatory, all other attributes are optional and can be omitted if not needed.</p>
<p>You specify the device type by providing the Device ID string. The following device types are supported:</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone">Device ID </th><th class="markdownTableHeadNone">Device  </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">"ORCA" </td><td class="markdownTableBodyNone">DeGirum Orca AI accelerator  </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">"EDGETPU" </td><td class="markdownTableBodyNone">Google Edge TPU AI accelerator  </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">"MYRIAD" </td><td class="markdownTableBodyNone">Intel Myriad AI accelerator  </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">"CPU" </td><td class="markdownTableBodyNone">AI server host computer  </td></tr>
</table>
<p>The <em>runtime agent</em> is the software module, which controls the device used for AI inference. You specify the runtime agent type by providing the Agent ID string. The following runtime agent types are supported:</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone">Agent ID </th><th class="markdownTableHeadNone">Agent  </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">"N2X" </td><td class="markdownTableBodyNone">DeGirum N2X runtime agent. Used for all Orca devices and N2X model inference on CPU.  </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">"TFLITE" </td><td class="markdownTableBodyNone">Google TFLite runtime agent. Used for Edge TPU devices and direct TFlite model inference on CPU.  </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">"OPENVINO" </td><td class="markdownTableBodyNone">Intel OpenVINO runtime agent. Used for Myriad devices and direct OpenVINO model inference on CPU.  </td></tr>
</table>
<p><em>Note: the runtime agent attribute is rarely required because in most use cases the AI hardware uniquely defines the runtime agent to use, so it is recommended to avoid setting this attribute.</em></p>
<p>Examples:</p>
<p>Find a model which has 'MobileNet_v2' substring in the model name: </p><pre class="fragment">    auto model_id = DG::modelFind( server, { "mobilenet_v2" } );
</pre><p>Find a model which has 'mobilenet_v2' substring in the model name, is designed to run on DeGirum Orca AI accelerator hardware, is quantized, and is sparse/pruned: </p><pre class="fragment">    auto model_id = DG::modelFind( server, { "MobileNet_v2", 
        "ORCA", "", DG::ModelQuery::Yes, DG::ModelQuery::Yes } );
</pre><h2><a class="anchor" id="ssec_cloud_zoo"></a>
Working with Cloud Zoo Models</h2>
<p>Starting from release 0.5.0, the AI server supports inference of models from DeGrirum-hosted cloud model zoos. In order to use models from a cloud zoo you need to register at DeGirum AI Hub, <a href="https://hub.degirum.com,">https://hub.degirum.com,</a> and generate API access token. Please refer either to PySDK documentation or General Information section, both available on <a href="https://hub.degirum.com,">https://hub.degirum.com,</a> for more details.</p>
<p>To do the inference of a model from some cloud zoo you need to perform the following steps:</p>
<ol type="1">
<li>Obtain your <b>API access token</b>. You can generate it on DeGirum AI Hub site <a href="https://hub.degirum.com">https://hub.degirum.com</a> under Management | My Tokens* main menu item.</li>
<li>Figure out the <b>extended model name</b>. It has the following format: "&lt;organization&gt;/&lt;zoo&gt;/&lt;model&gt;". The "&lt;organization&gt;" field is the name of the zoo owner organization, the "&lt;zoo&gt;" field is the name of the cloud zoo, and the "&lt;model&gt;" field is the name of the model in that cloud zoo. You can obtain the "&lt;organization&gt;/&lt;zoo&gt;" prefix (so-called model zoo path) on DeGirum AI Hub site <a href="https://hub.degirum.com">https://hub.degirum.com</a> under <em>Management | Models</em> main menu item. Just select the model zoo you want to access: the model zoo page opens. Then click the <code>Copy</code> button near the model zoo name to copy the model zoo path into the clipboard. All models available in the cloud zoo are also listed on the model zoo page. Click on particular model: the model page opens. The model name will be displayed at the top of the page. Now concatenate the model zoo path with the model name separated by the slash <code>/</code> to obtain the extended model name.</li>
<li>Create <a class="el" href="namespaceDG.html#a1386bac643ff32870b2bbedc788585d6" title="ModelParamsWriter is ModelParams template instantiation with write access.">DG::ModelParamsWriter</a> object and assign <code>CloudToken</code> parameter.</li>
<li>Create <a class="el" href="classDG_1_1AIModel.html" title="AIModel is DeGirum AI client API class for simple non-pipelined sequential inference.">DG::AIModel</a> or <a class="el" href="classDG_1_1AIModelAsync.html" title="AIModelAsync is DeGirum AI client API class for efficient pipelined asynchronous inference.">DG::AIModelAsync</a> object passing the AI server hostname, extended model name, and <a class="el" href="namespaceDG.html#a1386bac643ff32870b2bbedc788585d6" title="ModelParamsWriter is ModelParams template instantiation with write access.">DG::ModelParamsWriter</a> object.</li>
</ol>
<p>For example: const std::string hostname = ""; // specify AI server hostname const std::string ext_model_name = ""' // specify extended model name const std::string token = ""; // specify your API access token string</p>
<p><a class="el" href="namespaceDG.html#a1386bac643ff32870b2bbedc788585d6" title="ModelParamsWriter is ModelParams template instantiation with write access.">DG::ModelParamsWriter</a> mparams; mparams.CloudToken_set( token ); <a class="el" href="classDG_1_1AIModel.html" title="AIModel is DeGirum AI client API class for simple non-pipelined sequential inference.">DG::AIModel</a> model( hostname, ext_model_name, mparams );</p>
<p>AI server will automatically download the model from the cloud model zoo using provided credentials and store this model in the local model cache to be reused for later inferences.</p>
<p>Each time you create new <a class="el" href="classDG_1_1AIModel.html" title="AIModel is DeGirum AI client API class for simple non-pipelined sequential inference.">DG::AIModel</a> or <a class="el" href="classDG_1_1AIModelAsync.html" title="AIModelAsync is DeGirum AI client API class for efficient pipelined asynchronous inference.">DG::AIModelAsync</a> object and perform the first inference, the checksum of the model in the local cache is compared with the checksum of the model in the cloud zoo, and if they mismatch, the model from the cloud zoo is downloaded into the local cache. This mechanism guarantees that you always perform the inference of the newest model.</p>
<h2><a class="anchor" id="ssec_AIModel"></a>
AIModel: Synchronous Sequential AI Inferencing API</h2>
<p>The <a class="el" href="classDG_1_1AIModel.html" title="AIModel is DeGirum AI client API class for simple non-pipelined sequential inference.">DG::AIModel</a> class is used to perform AI model inference on AI server in simple non-pipelined sequential manner.</p>
<p><em>Note: for more efficient (but somewhat more complex to use) asynchronous pipelined inference use <a class="el" href="classDG_1_1AIModelAsync.html" title="AIModelAsync is DeGirum AI client API class for efficient pipelined asynchronous inference.">DG::AIModelAsync</a> class.</em></p>
<p>On construction this class performs connection to AI server, selection of AI model, and setting model run-time parameters.</p>
<p>You specify the AI server address as a string as described in the section <a class="el" href="index.html#ssec_server_addressing">AI Server Connection and Addressing</a>.</p>
<p>You specify the model name as described in the section <a class="el" href="index.html#ssec_zoo_search">Listing and Searching Models in Local Model Zoo</a>.</p>
<p>If you want to modify some of the model parameters (see more on that in the section <a class="el" href="index.html#ssec_params">Model Parameters Management</a>), you define the instance of ModelParamsWriteAccess-derived class, change model parameters you want to adjust by invoking setter methods, and pass this instance as one of <a class="el" href="classDG_1_1AIModel.html" title="AIModel is DeGirum AI client API class for simple non-pipelined sequential inference.">DG::AIModel</a> constructor arguments.</p>
<p>For example, you want to change the input frame image type to JPEG. You can pass the following expression to <a class="el" href="classDG_1_1AIModel.html" title="AIModel is DeGirum AI client API class for simple non-pipelined sequential inference.">DG::AIModel</a> constructor: </p><pre class="fragment">    DG::ModelParamsWriter().InputImgFmt_set( "JPEG" )
</pre><p>Once constructed, it can be used to perform sequential AI inference by invoking <a class="el" href="classDG_1_1AIModel.html#ad7a35ec0ba3e9e4fa2312718ba959c86">DG::AIModel::predict()</a> method. Predict method accepts the input frame data in various formats (see section <a class="el" href="index.html#ssec_format">Format of Data Frames for Image Processing Models</a> for more details) and returns the inference results, which are described in the section <a class="el" href="index.html#ssec_inference_results">Format of Inference Results</a>. This method is blocking method, i.e. it returns the execution only when the inference of an input frame is complete. Working with blocking methods simplifies the programming, but loses some run-time efficiency because (unlike for <a class="el" href="classDG_1_1AIModelAsync.html" title="AIModelAsync is DeGirum AI client API class for efficient pipelined asynchronous inference.">DG::AIModelAsync</a> class) some actions like sending data to AI server, frame pre-processing, result post-processing, and receiving results from AI server are not pipelined between frames.</p>
<h2><a class="anchor" id="ssec_inference_results"></a>
Format of Inference Results</h2>
<p>The predict() method of <a class="el" href="classDG_1_1AIModel.html" title="AIModel is DeGirum AI client API class for simple non-pipelined sequential inference.">DG::AIModel</a> class returns JSON array with inference results. The internal structure of this JSON array greatly depends on the AI model type: it is different for classification models, detection models, and so on. But in general, that JSON array is an array of identical JSON objects, and each JSON object contains inference results for one detected entity.</p>
<p>The following tables describe JSON array format for some model types.</p>
<p>For classification model types each classified object is represented by the following JSON object:</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone">JSON Field Name </th><th class="markdownTableHeadNone">Description </th><th class="markdownTableHeadNone">Data Type  </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">"label" </td><td class="markdownTableBodyNone">Classified object class label </td><td class="markdownTableBodyNone">string  </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">"category_id" </td><td class="markdownTableBodyNone">Classified object class index </td><td class="markdownTableBodyNone">integer  </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">"score" </td><td class="markdownTableBodyNone">Probability of classified object </td><td class="markdownTableBodyNone">floating point  </td></tr>
</table>
<p>For detection model types each detected object is represented by the following JSON object:</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone">JSON Field Name </th><th class="markdownTableHeadNone">Description </th><th class="markdownTableHeadNone">Data Type  </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">"label" </td><td class="markdownTableBodyNone">Detected object class label </td><td class="markdownTableBodyNone">string  </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">"category_id" </td><td class="markdownTableBodyNone">Detected object class index </td><td class="markdownTableBodyNone">integer  </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">"score" </td><td class="markdownTableBodyNone">Probability of detected object </td><td class="markdownTableBodyNone">floating point  </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">"bbox" </td><td class="markdownTableBodyNone">Detected object rectangle </td><td class="markdownTableBodyNone">four-element JSON array [xmin, ymin, xmax, ymax]  </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">"landmarks" </td><td class="markdownTableBodyNone">Landmark points (optional) </td><td class="markdownTableBodyNone">JSON array of two-element arrays [[x1, y1], [x2, y2], ...]  </td></tr>
</table>
<p>For pose detection types each detected object is represented by the following JSON object:</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone">JSON Field Name </th><th class="markdownTableHeadNone">Description </th><th class="markdownTableHeadNone">Data Type  </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">"score" </td><td class="markdownTableBodyNone">Probability of detected pose </td><td class="markdownTableBodyNone">floating point  </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">"landmarks" </td><td class="markdownTableBodyNone">Pose description landmarks </td><td class="markdownTableBodyNone">array of landmark JSON objects  </td></tr>
</table>
<p>The following is the structure of pose description landmark JSON object:</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone">JSON Field Name </th><th class="markdownTableHeadNone">Description </th><th class="markdownTableHeadNone">Data Type  </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">"label" </td><td class="markdownTableBodyNone">Classified object class label </td><td class="markdownTableBodyNone">string  </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">"category_id" </td><td class="markdownTableBodyNone">Classified object class index </td><td class="markdownTableBodyNone">integer  </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">"score" </td><td class="markdownTableBodyNone">Probability of classified landmark </td><td class="markdownTableBodyNone">floating point  </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">"landmark" </td><td class="markdownTableBodyNone">landmark point </td><td class="markdownTableBodyNone">two-element JSON array [x, y]  </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">"connect" </td><td class="markdownTableBodyNone">list of adjacent landmarks indices </td><td class="markdownTableBodyNone">JSON array of integers  </td></tr>
</table>
<p>For hand detection types each detected object is represented by the following JSON object:</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone">JSON Field Name </th><th class="markdownTableHeadNone">Description </th><th class="markdownTableHeadNone">Data Type  </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">"score" </td><td class="markdownTableBodyNone">Probability of detected hand </td><td class="markdownTableBodyNone">floating point  </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">"handedness" </td><td class="markdownTableBodyNone">Probability of right hand </td><td class="markdownTableBodyNone">floating point  </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">"landmarks" </td><td class="markdownTableBodyNone">hand description landmarks </td><td class="markdownTableBodyNone">array of landmark JSON objects  </td></tr>
</table>
<p>The following is the structure of hand description landmark JSON object:</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone">JSON Field Name </th><th class="markdownTableHeadNone">Description </th><th class="markdownTableHeadNone">Data Type  </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">"label" </td><td class="markdownTableBodyNone">Classified object class label </td><td class="markdownTableBodyNone">string  </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">"category_id" </td><td class="markdownTableBodyNone">Classified object class index </td><td class="markdownTableBodyNone">integer  </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">"landmark" </td><td class="markdownTableBodyNone">landmark point </td><td class="markdownTableBodyNone">three-element JSON array [x, y, z]  </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">"world_landmark" </td><td class="markdownTableBodyNone">metric world landmark point </td><td class="markdownTableBodyNone">three-element JSON array [x, y, z]  </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">"connect" </td><td class="markdownTableBodyNone">list of adjacent landmarks indices </td><td class="markdownTableBodyNone">JSON array of integers  </td></tr>
</table>
<p>When you specify post-processing type as "None" (see section <a class="el" href="index.html#ssec_params">Model Parameters Management</a>) then the inference result JSON array contains network output tensors. Each element of that array corresponds to one tensor. Each tensor is represented by the following JSON object:</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone">JSON Field Name </th><th class="markdownTableHeadNone">Description </th><th class="markdownTableHeadNone">Data Type  </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">"id" </td><td class="markdownTableBodyNone">Tensor numeric ID as specified in network </td><td class="markdownTableBodyNone">integer  </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">"name" </td><td class="markdownTableBodyNone">Tensor name as specified in network </td><td class="markdownTableBodyNone">string  </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">"shape" </td><td class="markdownTableBodyNone">Tensor shape: sizes of each dimension </td><td class="markdownTableBodyNone">integer array  </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">"quantization" </td><td class="markdownTableBodyNone">Tensor quantization parameters </td><td class="markdownTableBodyNone">object  </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">"type" </td><td class="markdownTableBodyNone">Tensor element type </td><td class="markdownTableBodyNone">string  </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">"data" </td><td class="markdownTableBodyNone">Tensor linear data buffer contents </td><td class="markdownTableBodyNone">binary field  </td></tr>
</table>
<p>The following tensor data types are supported:</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone">Type String </th><th class="markdownTableHeadNone">Type Description  </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">DG_FLT </td><td class="markdownTableBodyNone">32-bit floating point  </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">DG_UINT8 </td><td class="markdownTableBodyNone">8-bit unsigned integer  </td></tr>
</table>
<p>The following is the structure of quantization JSON object:</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone">JSON Field Name </th><th class="markdownTableHeadNone">Description </th><th class="markdownTableHeadNone">Data Type  </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">"axis" </td><td class="markdownTableBodyNone">Quantization axis or -1 for global quantization </td><td class="markdownTableBodyNone">integer  </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">"scale" </td><td class="markdownTableBodyNone">Quantization scale array </td><td class="markdownTableBodyNone">floating point array  </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">"zero" </td><td class="markdownTableBodyNone">Quantization zero offset </td><td class="markdownTableBodyNone">integer array  </td></tr>
</table>
<p>For convenient handling of tensors the <a class="el" href="classDG_1_1BasicTensor.html">DG::BasicTensor</a> class is provided. You can convert JSON array containing a tensor into <a class="el" href="classDG_1_1BasicTensor.html">DG::BasicTensor</a> object by using <a class="el" href="classDG_1_1JsonHelper.html#a989825a70022be842aab01060b828e15">DG::JsonHelper::tensorDeserialize()</a> static method: </p><pre class="fragment">    for( const auto &amp;json_tensor: inference_result )
    {
        DG::BasicTensor basic_tensor( DG::JsonHelper::tensorDeserialize( json_tensor ) );
        auto shape = basic_tensor.shape();              // get tensor shape
        auto element_type = basic_tensor.dataTypeGet(); // get tensor element type
        if( element_type == DG_UINT8 )                  // get pointer to tensor linear buffer
            const uint8_t *data = basic_tensor.data&lt; uint8_t &gt;();
    }
</pre><p>There is a special case of JSON array returned by AI server in case when runtime error happens during inference. In this case the runtime error info object is returned. It is described in details in the <a class="el" href="index.html#ssec_errors">Error Handling</a> section.</p>
<h2><a class="anchor" id="ssec_AIModelAsync"></a>
AIModelAsync: Asynchronous Pipelined AI Inferencing API</h2>
<p>The <a class="el" href="classDG_1_1AIModelAsync.html" title="AIModelAsync is DeGirum AI client API class for efficient pipelined asynchronous inference.">DG::AIModelAsync</a> class is used to perform AI model inference on AI server in efficient pipelined asynchronous manner using the mechanism of callbacks.</p>
<p><em>Note: for simple (but less efficient) synchronous non-pipelined inference use <a class="el" href="classDG_1_1AIModel.html" title="AIModel is DeGirum AI client API class for simple non-pipelined sequential inference.">DG::AIModel</a> class.</em></p>
<p>On construction this class performs connection to AI server, selection of AI model, setting model run-time parameters, and installation of the client callback function. The client callback function is used to pass the inference results from the AI server to the client code.</p>
<p>You specify the AI server address as a string as described in the section <a class="el" href="index.html#ssec_server_addressing">AI Server Connection and Addressing</a>.</p>
<p>You specify the model name as described in the section <a class="el" href="index.html#ssec_zoo_search">Listing and Searching Models in Local Model Zoo</a>.</p>
<p>You should define the callback function somewhere in your code in the following form: </p><pre class="fragment">    void MyAiCallback( const json &amp;result, const std::string &amp;info ){}
</pre><p>Then you pass your callback name (in this example it is MyAiCallback) as one of <a class="el" href="classDG_1_1AIModelAsync.html" title="AIModelAsync is DeGirum AI client API class for efficient pipelined asynchronous inference.">DG::AIModelAsync</a> constructor arguments. The callback function has two arguments: the inference result and the frame info string. The inference result is JSON array, described in the section <a class="el" href="index.html#ssec_inference_results">Format of Inference Results</a>. The frame info string is the string passed to predict() method along with the frame data. More on that below.</p>
<p>If you want to modify some of the model parameters (see more on that in the section <a class="el" href="index.html#ssec_params">Model Parameters Management</a>), you define the instance of <a class="el" href="classDG_1_1ModelParamsWriteAccess.html" title="ModelParamsWriteAccess is read/write accessor to model parameters.">DG::ModelParamsWriteAccess</a> derived class, change model parameters you want to adjust by invoking setter methods, and pass this instance as one of <a class="el" href="classDG_1_1AIModelAsync.html" title="AIModelAsync is DeGirum AI client API class for efficient pipelined asynchronous inference.">DG::AIModelAsync</a> constructor arguments.</p>
<p>For example, you want to change the input frame image type to JPEG. You can pass the following expression to <a class="el" href="classDG_1_1AIModelAsync.html" title="AIModelAsync is DeGirum AI client API class for efficient pipelined asynchronous inference.">DG::AIModelAsync</a> constructor: </p><pre class="fragment">    DG::ModelParamsWriter().InputImgFmt_set( "JPEG" )
</pre><p>Once constructed, it can be used to perform asynchronous AI inference by invoking <a class="el" href="classDG_1_1AIModelAsync.html#a1b12692b190b36e7e0e366f0bb59fdef">DG::AIModelAsync::predict()</a> method. The predict method accept the input frame data in various formats (see section <a class="el" href="index.html#ssec_format">Format of Data Frames for Image Processing Models</a> for more details) and initiates the inference on the AI server. The method is a non-blocking method, i.e. it returns execution immediately after posting the frame data to the AI server. This allows calling this method in a loop without waiting for the inference results, achieving the maximum AI server utilization.</p>
<p>To simplify matching inference results and corresponding frames in the client callback, additional argument can be passed to predict() method. The frame_info argument is optional frame information string. It will be passed to the client callback along with the frame result. You can pass arbitrary information as the frame info, for example frame data filenames, or frame numbers, or any other frame identification information, which you need to know in the scope of client callback function in order to interpret inference results in respect to the input frames.</p>
<p>Once the inference of a frame is complete, and the inference result is received from the AI server, the client callback is invoked to dispatch the inference result. Such result handling via callback mechanism is performed in a thread, separate from the main execution thread. It means that the client callback function is called in asynchronous manner, thus the name of the class.</p>
<p>During construction you may also specify the depth of the internal frame queue. The internal frame queue works the following way. If predict() method is invoked too often and the number of non-processed ("outstanding") frames exceeds the specified queue depth, the consecutive call to predict() method will be blocked until the number of outstanding frames in the queue becomes smaller than the queue depth thus allowing to post one more frame.</p>
<p>The class destructor waits until all outstanding results are received and then closes the connection to AI server. You may use <a class="el" href="classDG_1_1AIModelAsync.html#a9c08f21e655345c17617df8d6d71774b">DG::AIModelAsync::waitCompletion()</a> to explicitly wait for completion of all outstanding inferences. It is a blocking call: it returns only when all outstanding frames are processed by AI server and all results are dispatched via client callback.</p>
<p>To monitor the number of outstanding inferences in real-time you use <a class="el" href="classDG_1_1AIModelAsync.html#a4262c7337d952d2543f8265791c0d535" title="Get the number of outstanding inference results posted so far.">DG::AIModelAsync::outstandingResultsCountGet()</a> method.</p>
<p>To check for runtime errors which happened during inferences you use <a class="el" href="classDG_1_1AIModelAsync.html#a25c1ada23af9b591a45171e9a795092b">DG::AIModelAsync::lastError()</a> method. If ever during consecutive calls to predict() method the AI server reported a run-time error, then this method will return the error message string, otherwise it returns an empty string. Alternatively, you may check for runtime errors directly in the callback function by invoking <a class="el" href="namespaceDG.html#a9929b406008257b1ff51b5470aa094af">errorCheck()</a> function on inference result argument.</p>
<p><em>Note: in case of server runtime error, all frames posted after the error was detected, will not be processed.</em></p>
<h2><a class="anchor" id="ssec_params"></a>
Model Parameters Management</h2>
<p>Each AI model in the model zoo comes with the model parameters file in JSON format. This JSON file defines all model parameters such as model name, model input dimensions, pre-processing parameters, post-processing parameters, etc. Some of the model parameters are fixed and cannot be altered without breaking the model behavior, and some of the model parameters can be changed to adjust the model behavior. Later parameters are called model <em>runtime parameters</em> and can be adjusted when constructing instances of model inference classes <a class="el" href="classDG_1_1AIModel.html" title="AIModel is DeGirum AI client API class for simple non-pipelined sequential inference.">DG::AIModel</a> and <a class="el" href="classDG_1_1AIModelAsync.html" title="AIModelAsync is DeGirum AI client API class for efficient pipelined asynchronous inference.">DG::AIModelAsync</a>.</p>
<p>To work with model parameters the following classes are provided:</p>
<ul>
<li><a class="el" href="classDG_1_1ModelParamsReadAccess.html" title="ModelParamsReadAccess is read-only accessor to model parameters.">DG::ModelParamsReadAccess</a></li>
<li><a class="el" href="classDG_1_1ModelParamsWriteAccess.html" title="ModelParamsWriteAccess is read/write accessor to model parameters.">DG::ModelParamsWriteAccess</a></li>
<li><a class="el" href="classDG_1_1ModelParams.html" title="ModelParams is model parameters collection with user-defined access rights.">DG::ModelParams</a></li>
<li><a class="el" href="namespaceDG.html#a1386bac643ff32870b2bbedc788585d6" title="ModelParamsWriter is ModelParams template instantiation with write access.">DG::ModelParamsWriter</a></li>
</ul>
<p><a class="el" href="classDG_1_1ModelParamsReadAccess.html" title="ModelParamsReadAccess is read-only accessor to model parameters.">DG::ModelParamsReadAccess</a> class provides programmatic type-safe read access to model parameters defined in JSON model configuration. It keeps non-owning const reference to underlying JSON array. For each model parameter it provides getter method, which name matches the parameter name as it appears in JSON array.</p>
<p><a class="el" href="classDG_1_1ModelParamsWriteAccess.html" title="ModelParamsWriteAccess is read/write accessor to model parameters.">DG::ModelParamsWriteAccess</a> inherits <a class="el" href="classDG_1_1ModelParamsReadAccess.html" title="ModelParamsReadAccess is read-only accessor to model parameters.">DG::ModelParamsReadAccess</a> and provides programmatic type-safe read <em>and write</em> access to model parameters defined in JSON model configuration. It keeps non-owning non-const reference to underlying JSON array. For each model parameter it provides both getter and setter methods. Getter method name matches the parameter name as it appears in JSON array. Setter method name is constructed from the parameter name by appending <code>_set</code> suffix. For example, <code>InputImgFmt_set()</code> Setter methods return reference to self, so they can be called in a daisy-chain manner, for example: </p><pre class="fragment">    params.InputImgFmt_set( "RAW" ).InputImgRawDataType_set( "DG_UINT8" );
</pre><p><a class="el" href="classDG_1_1ModelParams.html" title="ModelParams is model parameters collection with user-defined access rights.">DG::ModelParams</a> class provides programmatic type-safe access to model parameters defined in JSON model configuration while <em>owning</em> that JSON array. Access (read or write) is defined by the template parameter, which is used as the base class. You can use any <a class="el" href="classDG_1_1ModelParamsReadAccess.html" title="ModelParamsReadAccess is read-only accessor to model parameters.">DG::ModelParamsReadAccess</a>-derived base class: it can be <a class="el" href="classDG_1_1ModelParamsReadAccess.html" title="ModelParamsReadAccess is read-only accessor to model parameters.">DG::ModelParamsReadAccess</a> or <a class="el" href="classDG_1_1ModelParamsWriteAccess.html" title="ModelParamsWriteAccess is read/write accessor to model parameters.">DG::ModelParamsWriteAccess</a>.</p>
<p>And finally <a class="el" href="namespaceDG.html#a1386bac643ff32870b2bbedc788585d6" title="ModelParamsWriter is ModelParams template instantiation with write access.">DG::ModelParamsWriter</a> is <a class="el" href="classDG_1_1ModelParams.html" title="ModelParams is model parameters collection with user-defined access rights.">DG::ModelParams</a> template instantiation with write access. Instances of this class you typically use to pass adjusted runtime model parameters to constructors of inference classes.</p>
<p>The following table summarizes all runtime parameters which are accessible to the client code:</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone">Parameter Name </th><th class="markdownTableHeadNone">Type </th><th class="markdownTableHeadNone">Access </th><th class="markdownTableHeadNone">Description </th><th class="markdownTableHeadNone">Possible Values </th><th class="markdownTableHeadNone">Default Value  </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone"><code>CloudToken</code> </td><td class="markdownTableBodyNone">string </td><td class="markdownTableBodyNone">R/W </td><td class="markdownTableBodyNone">Cloud API access token </td><td class="markdownTableBodyNone">Valid cloud API access token </td><td class="markdownTableBodyNone">""  </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone"><code>EagerBatchSize</code> </td><td class="markdownTableBodyNone">size_t </td><td class="markdownTableBodyNone">R/W </td><td class="markdownTableBodyNone">The size of the batch to be used by device scheduler when inferencing this model. The batch is the number of consecutive frames before this model is switched to another model during batch predict. </td><td class="markdownTableBodyNone">1..80 </td><td class="markdownTableBodyNone">8  </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone"><code>InputImgFmt</code> </td><td class="markdownTableBodyNone">string </td><td class="markdownTableBodyNone">R/W </td><td class="markdownTableBodyNone">Input frame format: JPEG or raw binary </td><td class="markdownTableBodyNone">"JPEG", "RAW" </td><td class="markdownTableBodyNone">"JPEG"  </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone"><code>InputRawDataType</code> </td><td class="markdownTableBodyNone">string </td><td class="markdownTableBodyNone">R/W </td><td class="markdownTableBodyNone">Data type for raw binary frame format: byte of 32-bit float </td><td class="markdownTableBodyNone">"DG_UINT8", "DG_FLT" </td><td class="markdownTableBodyNone">"DG_UINT8"  </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone"><code>InputImgRotation</code> </td><td class="markdownTableBodyNone">int </td><td class="markdownTableBodyNone">R/W </td><td class="markdownTableBodyNone">Input image rotation angle in degrees, clockwise </td><td class="markdownTableBodyNone">0, 90, 180, 270 </td><td class="markdownTableBodyNone">0  </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone"><code>InputColorSpace</code> </td><td class="markdownTableBodyNone">string </td><td class="markdownTableBodyNone">R/W </td><td class="markdownTableBodyNone">Input image colorspace (sequence of colors in C dimension) </td><td class="markdownTableBodyNone">"RGB", "BGR" </td><td class="markdownTableBodyNone">"RGB"  </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone"><code>InputResizeMethod</code> </td><td class="markdownTableBodyNone">string </td><td class="markdownTableBodyNone">R/O </td><td class="markdownTableBodyNone">Interpolation algorithm used for image resizing during model training </td><td class="markdownTableBodyNone">"nearest", "bilinear", "area", "bicubic", "lanczos" </td><td class="markdownTableBodyNone">"bilinear"  </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone"><code>InputPadMethod</code> </td><td class="markdownTableBodyNone">string </td><td class="markdownTableBodyNone">R/O </td><td class="markdownTableBodyNone">How input image is padded when resized during model training </td><td class="markdownTableBodyNone">"stretch", "letterbox", "crop-first", "crop-last" </td><td class="markdownTableBodyNone">"letterbox"  </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone"><code>InputCropPercentage</code> </td><td class="markdownTableBodyNone">double </td><td class="markdownTableBodyNone">R/O </td><td class="markdownTableBodyNone">How much input image was cropped during model training </td><td class="markdownTableBodyNone">0..1 </td><td class="markdownTableBodyNone">1  </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone"><code>ImageBackend</code> </td><td class="markdownTableBodyNone">string </td><td class="markdownTableBodyNone">R/O </td><td class="markdownTableBodyNone">Graphical package used for image processing during model training </td><td class="markdownTableBodyNone">"pil", "opencv", "auto" </td><td class="markdownTableBodyNone">"auto"  </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone"><code>MaxDetections</code> </td><td class="markdownTableBodyNone">int </td><td class="markdownTableBodyNone">R/W </td><td class="markdownTableBodyNone">Maximum number of objects to report for detection models </td><td class="markdownTableBodyNone">&gt; 0 </td><td class="markdownTableBodyNone">20  </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone"><code>MaxDetectionsPerClass</code> </td><td class="markdownTableBodyNone">int </td><td class="markdownTableBodyNone">R/W </td><td class="markdownTableBodyNone">Maximum number of objects to report for each class for detection models </td><td class="markdownTableBodyNone">&gt; 0 </td><td class="markdownTableBodyNone">100  </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone"><code>MaxClassesPerDetection</code> </td><td class="markdownTableBodyNone">int </td><td class="markdownTableBodyNone">R/W </td><td class="markdownTableBodyNone">Maximum number of classes to report for detection models </td><td class="markdownTableBodyNone">&gt; 0 </td><td class="markdownTableBodyNone">30  </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone"><code>MeasureTime</code> </td><td class="markdownTableBodyNone">bool </td><td class="markdownTableBodyNone">R/W </td><td class="markdownTableBodyNone">Enable inference time measurements </td><td class="markdownTableBodyNone">true/false </td><td class="markdownTableBodyNone">false  </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone"><code>OutputPostprocessType</code> </td><td class="markdownTableBodyNone">string </td><td class="markdownTableBodyNone">R/W </td><td class="markdownTableBodyNone">Post-processor type </td><td class="markdownTableBodyNone">see below </td><td class="markdownTableBodyNone">"None"  </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone"><code>OutputConfThreshold</code> </td><td class="markdownTableBodyNone">double </td><td class="markdownTableBodyNone">R/W </td><td class="markdownTableBodyNone">Confidence threshold </td><td class="markdownTableBodyNone">0..1 </td><td class="markdownTableBodyNone">0.1  </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone"><code>OutputNMSThreshold</code> </td><td class="markdownTableBodyNone">double </td><td class="markdownTableBodyNone">R/W </td><td class="markdownTableBodyNone">Threshold for non-max suppression </td><td class="markdownTableBodyNone">0..1 </td><td class="markdownTableBodyNone">0.6  </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone"><code>OutputTopK</code> </td><td class="markdownTableBodyNone">size_t </td><td class="markdownTableBodyNone">R/W </td><td class="markdownTableBodyNone">Number of classes with biggest scores to report for classification models </td><td class="markdownTableBodyNone">0,1... </td><td class="markdownTableBodyNone">0  </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone"><code>UseRegularNMS</code> </td><td class="markdownTableBodyNone">bool </td><td class="markdownTableBodyNone">R/W </td><td class="markdownTableBodyNone">Use regular (per-class) NMS algorithm as opposed to global (class-ignoring) NMS algorithm for detection models </td><td class="markdownTableBodyNone">true/false </td><td class="markdownTableBodyNone">true  </td></tr>
</table>
<p>If you do not specify a runtime parameter, then the value for this parameter will be taken from the model JSON configuration file from the local model zoo. If the model JSON configuration file also does not specify this parameter, then the default value will be used.</p>
<p>The following table lists currently available post-processor types.</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone">Post-Processor Name </th><th class="markdownTableHeadNone">Applicability </th><th class="markdownTableHeadNone">Description  </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">"None" </td><td class="markdownTableBodyNone">All models </td><td class="markdownTableBodyNone">Pass-through post-processor. Network output tensors are sent without any post-processing  </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">"Classification" </td><td class="markdownTableBodyNone">Classification models </td><td class="markdownTableBodyNone">Classification post-processor for all classification models  </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">"Segmentation" </td><td class="markdownTableBodyNone">Segmentation models </td><td class="markdownTableBodyNone">Semantic segmentation post-processor for all segmentation models  </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">"TFLiteDetection" </td><td class="markdownTableBodyNone">TFlite detection models </td><td class="markdownTableBodyNone">Object detection post-processor for TFLite detection models  </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">"DetectionYolo" </td><td class="markdownTableBodyNone">Yolov5-based models </td><td class="markdownTableBodyNone">Object detection post-processor for DeGirum Yolov5-based models  </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">"Detection" </td><td class="markdownTableBodyNone">Other detection models </td><td class="markdownTableBodyNone">Object detection post-processor for all other DeGirum general object detection models  </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">"DetectionYoloPlates" </td><td class="markdownTableBodyNone">Yolov5-based license plate detection models </td><td class="markdownTableBodyNone">DeGirum license plate detection model post-processor  </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">"DetectionYoloV8Plates" </td><td class="markdownTableBodyNone">Yolov8-based license plate detection models </td><td class="markdownTableBodyNone">DeGirum license plate detection model post-processor  </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">"DetectionYoloV8OBB" </td><td class="markdownTableBodyNone">Yolov8-based oriented bounding box (OBB) detection models </td><td class="markdownTableBodyNone">OBB object detection post-processor for DeGirum Yolov8-based OBB models  </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">"FaceDetection" </td><td class="markdownTableBodyNone">Face detection models </td><td class="markdownTableBodyNone">Face detection post-processor for all DeGirum face detection models  </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">"PoseDetection" </td><td class="markdownTableBodyNone">Pose detection models </td><td class="markdownTableBodyNone">Person pose detection post-processor for all DeGirum pose detection models  </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">"HandDetection" </td><td class="markdownTableBodyNone">Hand detection models </td><td class="markdownTableBodyNone">Hand landmark detection post-processor for all DeGirum hand detection models  </td></tr>
</table>
<p><em>Note: not all post-processors are compatible with all models. Special care should be taken when specifying post-processor type other than "None".</em></p>
<p>Notes of Input* parameters.</p>
<p>DG::ModelParamsReadAccess::InputImgFmt parameter is used to tell pre-processor what input frame format to expect. The value of this parameter should match the frame data type you pass to predict() methods. You may change it to match the format of your input frames.</p>
<p>DG::ModelParamsReadAccess::InputRawDataType parameter is used to tell pre-processor what data element type to expect for raw data frames (InputImgFmt is "RAW"). Again, the value of this parameter should match the frame data element type you pass to predict() methods. You may change it to match the format of your input frames.</p>
<p>DG::ModelParamsReadAccess::InputImgRotation parameter is used to ask pre-processor to perform input image rotation by certain angle. It is convenient when your input frames are coming from your camera which is mounted on a side or upside down.</p>
<p>DG::ModelParamsReadAccess::InputColorSpace defines the colorspace (pixel order) which the AI model expects (was trained to). In case when the input image format is "RAW", you should provide raw image frame with correct pixel order corresponding to the input colorspace parameter value. In case when the input image format is "JPEG", the pre-processor will perform the conversion to the proper pixel order when decoding the JPEG image. Usually you do not change this parameter, otherwise the model performance will degrade.</p>
<h2><a class="anchor" id="ssec_format"></a>
Format of Data Frames for Image Processing Models</h2>
<p>The AI Client package supports two formats of the input data frames for image processing models: JPEG images and binary bitmaps, where binary format is used by default and typically set in JSON model configuration files for all DeGirum models.</p>
<p>In any case the HxW dimension of the input data frame must match the input dimension of the AI model used for the inference. The input dimensions of the AI model are stored in the <a class="el" href="structDG_1_1ModelInfo.html" title="ModelInfo is the model identification structure. It keeps AI model key attributes.">DG::ModelInfo</a> data structure members <code>DG::ModelInfo::extended_params.InputW()</code> and <code>DG::ModelInfo::extended_params::InputH()</code>, returned by either <a class="el" href="namespaceDG.html#a482a6ce0ec392df3b6f26db70b9b941e">modelzooListGet()</a> or <a class="el" href="namespaceDG.html#ae6fb2ad5460b7160220feb74635845a1">modelFind()</a> functions (see section <a class="el" href="index.html#ssec_zoo_search">Listing and Searching Models in Local Model Zoo</a>).</p>
<p>The frame data in the binary bitmap format should be arranged as HWC three-dimensional dense array, meaning that color dimension 'C' is the fastest dimension, while the vertical dimension 'H' is the slowest. The color order is dependent on how the model was originally trained. The pixel data type can be either 8-bit unsigned integer number or 32-bit floating point number.</p>
<p>You specify input frame type when you construct instances of model inference classes <a class="el" href="classDG_1_1AIModel.html" title="AIModel is DeGirum AI client API class for simple non-pipelined sequential inference.">DG::AIModel</a> and <a class="el" href="classDG_1_1AIModelAsync.html" title="AIModelAsync is DeGirum AI client API class for efficient pipelined asynchronous inference.">DG::AIModelAsync</a>, passing instance of ModelParams class as the constructor parameter (see more details in the section <a class="el" href="index.html#ssec_params">Model Parameters Management</a>).</p>
<p>To specify JPEG frame type you need to set InputImgFmt parameter to "JPEG".</p>
<p>To specify binary frame type you need to set both InputImgFmt and InputImgRawDataType parameters. For example, the byte binary frame type you set this way: </p><pre class="fragment">    std::string server = "192.168.0.1"; // some server address
    auto model_id = DG::modelFind( server, { "MobileNet_v2" } );
    DG::AIModel model( server, model_id, 
        DG::ModelParamsWriter.InputImgFmt_set( "RAW" ).InputImgRawDataType_set( "DG_UINT8" ) );
</pre><h2><a class="anchor" id="ssec_errors"></a>
Error Handling</h2>
<p>There are two mechanisms of error handling employed in AI Client package: using standard library exceptions and using specially formatted JSON inference results.</p>
<p>All runtime errors <em>except errors happened during inference</em>, are handled by throwing standard library exceptions - objects of std::exception type. To handle such errors the recommended practice is to place all the code which interacts with the AI Client library into one try/catch block, and then catch exceptions of std::exception type: </p><pre class="fragment">    try
    {
        ...
    }
    catch( std::exception &amp;e )
    {
        std::cout &lt;&lt; e.what();  // handle exception 
    }
</pre><p>Errors which happened during the inference are handled special way. Instead of throwing an exception, the special JSON runtime error info object is returned by <a class="el" href="classDG_1_1AIModel.html#ad7a35ec0ba3e9e4fa2312718ba959c86">DG::AIModel::predict()</a> method in case you use synchronous API, and that JSON object is passed as an argument into your callback in case you use asynchronous API.</p>
<p>The JSON runtime error info object has the following structure:</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone">JSON Field Name </th><th class="markdownTableHeadNone">Description  </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">"success" </td><td class="markdownTableBodyNone">Boolean field containing "false" in case of error  </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">"msg" </td><td class="markdownTableBodyNone">Error message string  </td></tr>
</table>
<p>If you want to check that the inference result actually defines runtime error, you use <a class="el" href="namespaceDG.html#a9929b406008257b1ff51b5470aa094af">errorCheck()</a> function. If the AI server JSON response contains the run-time error, then this function will return the error message string, otherwise it returns an empty string. The following code demonstrates the typical use case for synchronous API: </p><pre class="fragment">    json result;
    model.predict( frame, result );
    auto possible_error = DG::errorCheck( result );
    if( !possible_error.empty() )
        throw std::runtime_error( possible_error );
</pre><p>The following code demonstrates the typical use case for asynchronous API: </p><pre class="fragment">    void MyAiCallback( const json &amp;result, const std::string &amp;info )
    {
        auto possible_error = DG::errorCheck( result );
        if( possible_error.empty() )
        {
            ... accept inference result
        }
        else // do not throw exception, since it will be ignored by AI Client thread
    }
</pre><p><em>Note: do not throw any exceptions in your callback, since the AI Client callback thread, which handles user callback invocation, suppresses all exceptions thrown in callbacks to not to terminate the callback thread unexpectedly.</em></p>
<p>Another approach for the inference error handling for asynchronous API would be to check for inference errors at the very end of the inference sequence. You can use <a class="el" href="classDG_1_1AIModelAsync.html#a25c1ada23af9b591a45171e9a795092b">DG::AIModelAsync::lastError()</a> method: if ever during consecutive calls to predict() method the inference errors are detected, then this method will return the error message string, otherwise it returns an empty string. To guarantee that all outstanding frames are processed before the error check, you can use <a class="el" href="classDG_1_1AIModelAsync.html#a9c08f21e655345c17617df8d6d71774b">DG::AIModelAsync::waitCompletion()</a> method: </p><pre class="fragment">    model.waitCompletion();
    auto possible_error = model.lastError();
    if( !possible_error.empty() )
        throw std::runtime_error( possible_error );
</pre><p><em>Note: in case of inference error in asynchronous client, all frames posted after the error was detected, will not be processed.</em> </p>
</div></div><!-- PageDoc -->
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.17
</small></address>
</body>
</html>
